Â¡Perfecto! Empezar con Kubernetes puede ser desafiante, pero con esta guÃ­a vamos paso a paso y aprenderÃ¡s muchÃ­simo en el proceso.

ğŸš€ **Â¿QuÃ© vamos a hacer?**

Vas a montar tu propio clÃºster de Kubernetes desde cero, a mano, usando el tutorial â€œKubernetes The Hard Wayâ€. Es ideal si quieres aprender en profundidad cÃ³mo funciona.

ğŸ› ï¸ **Requisitos iniciales**

Para comenzar, necesitas 4 mÃ¡quinas (fÃ­sicas o virtuales), todas con Debian 12 (bookworm). Pueden ser:

*   MÃ¡quinas virtuales con VirtualBox, VMware, Proxmox, QEMU, o en la nube (DigitalOcean, AWS, etc.).
*   Raspberry Pis (si estÃ¡s en ARM64).
*   Tu propio equipo, usando herramientas como multipass o Vagrant.

Las 4 mÃ¡quinas serÃ¡n:

| Nombre  | Rol                     | CPU | RAM   | Disco | Notas                       |
| :------ | :---------------------- | :-- | :---- | :---- | :-------------------------- |
| jumpbox | MÃ¡quina de administraciÃ³n | 1   | 512MB | 10GB  | Desde aquÃ­ ejecutamos todo  |
| server  | Plano de control        | 1   | 2GB   | 20GB  | Ejecuta API, etcd, etc.     |
| node-0  | Nodo de trabajo         | 1   | 2GB   | 20GB  | Ejecuta pods                |
| node-1  | Nodo de trabajo         | 1   | 2GB   | 20GB  | Ejecuta pods                |

ğŸ§­ **PASO A PASO**

Te ayudarÃ© paso por paso, empezando por el primero.

âœ… **Paso 1: Prepara las mÃ¡quinas**

Asumimos que ya tienes las 4 mÃ¡quinas corriendo Debian 12 con acceso SSH como root. Â¡Excelente! ğŸ‰ Ya con las 4 mÃ¡quinas listas, ahora comenzamos oficialmente el tutorial.

âœ… **Paso 2: Configurar el jumpbox**

Tu mÃ¡quina `jumpbox` serÃ¡ tu centro de control. Desde aquÃ­ ejecutarÃ¡s casi todos los comandos.

ğŸ”§ **1. Accede como root**

En tu terminal:

```bash
ssh root@IP_DEL_JUMPBOX
```

(reemplaza `IP_DEL_JUMPBOX` con la IP real de tu jumpbox)

ğŸ”§ **2. Instala herramientas bÃ¡sicas**

Una vez dentro de la mÃ¡quina:

```bash
apt-get update
apt-get install -y wget curl vim openssl git
```

ğŸ”§ **3. Clona el repositorio del tutorial**

```bash
git clone --depth 1 https://github.com/kelseyhightower/kubernetes-the-hard-way.git
cd kubernetes-the-hard-way
```

Este directorio (`/root/kubernetes-the-hard-way`) serÃ¡ donde trabajes durante toda la instalaciÃ³n.

ğŸ”§ **4. Descarga los binarios**

Ya que usas ARM64 o AMD64, detectamos automÃ¡ticamente quÃ© descargar:

```bash
ARCH=$(dpkg --print-architecture)
wget -q --show-progress --https-only --timestamping \
  -P downloads -i downloads-${ARCH}.txt
```

Luego extrae y organiza los binarios:

```bash
mkdir -p downloads/{client,cni-plugins,controller,worker}

tar -xvf downloads/crictl-v*.tar.gz -C downloads/worker/
tar -xvf downloads/containerd-v*.tar.gz --strip-components=1 -C downloads/worker/
tar -xvf downloads/cni-plugins-linux-${ARCH}-v*.tgz -C downloads/cni-plugins/
tar -xvf downloads/etcd-v*.tar.gz -C downloads/ \
  --strip-components 1 etcd-v*/etcdctl etcd-v*/etcd

mv downloads/{etcdctl,kubectl} downloads/client/
mv downloads/{etcd,kube-apiserver,kube-controller-manager,kube-scheduler} downloads/controller/
mv downloads/{kubelet,kube-proxy} downloads/worker/
mv downloads/runc.${ARCH} downloads/worker/runc # Ajustado para que coincida con el original

chmod +x downloads/{client,cni-plugins,controller,worker}/*
rm -rf downloads/*gz # Limpia los archivos tar.gz descargados
```

ğŸ”§ **5. Instala kubectl en el sistema**

```bash
cp downloads/client/kubectl /usr/local/bin/
```

Verifica que funcione:

```bash
kubectl version --client
```

Debe mostrar algo como:

```
Client Version: v1.32.3
```

âœ… **Paso 3: Provisionar las mÃ¡quinas (server, node-0, node-1)**

ğŸ”¹ **Objetivo**

*   Crear un archivo `machines.txt` con info de tus nodos.
*   Configurar acceso SSH por clave pÃºblica desde el `jumpbox` a `server`, `node-0` y `node-1`.
*   Asignar nombres (hostname) a cada mÃ¡quina (`server`, `node-0`, `node-1`).
*   Actualizar `/etc/hosts` en todas las mÃ¡quinas (`jumpbox`, `server`, `node-0`, `node-1`) para que todas puedan resolverse entre sÃ­ por nombre.

ğŸ§¾ **1. Crea el archivo `machines.txt`**

Desde tu `jumpbox`, en el directorio `kubernetes-the-hard-way`, crea el archivo con este formato:

```
IPV4_ADDRESS FQDN HOSTNAME POD_SUBNET
```

Ejemplo (reemplaza las IPs y FQDNs con los tuyos si son diferentes):

```bash
cat > machines.txt <<EOF
192.168.1.10 server.kubernetes.local server
192.168.1.11 node-0.kubernetes.local node-0 10.200.0.0/24
192.168.1.12 node-1.kubernetes.local node-1 10.200.1.0/24
EOF
```

âœï¸ Modifica las IPs, FQDNs y subredes si las tuyas son distintas. La columna `POD_SUBNET` solo es necesaria para los nodos worker.

ğŸ”‘ **2. Habilitar acceso SSH como root (si no estÃ¡ ya habilitado)**

En cada nodo (`server`, `node-0`, `node-1`), haz esto si es necesario:

Accede a cada mÃ¡quina con tu usuario normal y luego cambia a root:
`su - root`

Luego edita `/etc/ssh/sshd_config`:

```bash
sed -i 's/^#*PermitRootLogin.*/PermitRootLogin yes/' /etc/ssh/sshd_config
systemctl restart sshd
```

Esto permite que puedas conectar desde el `jumpbox` como root.

ğŸ” **3. Generar y distribuir claves SSH (desde el jumpbox)**

En tu `jumpbox`:

```bash
ssh-keygen -t rsa -b 4096 -N "" -f ~/.ssh/id_rsa
```

Ahora copia la clave pÃºblica a los nodos (`server`, `node-0`, `node-1`):

```bash
while read IP FQDN HOST SUBNET; do
  ssh-copy-id root@${IP}
done < machines.txt
```

Verifica el acceso SSH sin contraseÃ±a:

```bash
while read IP FQDN HOST SUBNET; do
  ssh -n root@${IP} hostname
done < machines.txt
```

Debe devolver:

```
server
node-0
node-1
```

ğŸ·ï¸ **4. Configura los hostnames (desde el jumpbox)**

```bash
while read IP FQDN HOST SUBNET; do
  ssh -n root@${IP} "sed -i 's/^127.0.1.1.*/127.0.1.1\t${FQDN} ${HOST}/' /etc/hosts"
  ssh -n root@${IP} "hostnamectl set-hostname ${HOST}"
  ssh -n root@${IP} "systemctl restart systemd-hostnamed"
done < machines.txt
```

Verifica:

```bash
while read IP FQDN HOST SUBNET; do
  ssh -n root@${IP} hostname --fqdn
done < machines.txt
```

Debe devolver:

```
server.kubernetes.local
node-0.kubernetes.local
node-1.kubernetes.local
```

ğŸ§­ **5. Configura `/etc/hosts` en todas las mÃ¡quinas (jumpbox, server, node-0, node-1)**

En el `jumpbox`, crea un archivo `hosts` temporal que contenga las entradas para todas las mÃ¡quinas del clÃºster:

```bash
echo "" > hosts
echo "# Kubernetes The Hard Way Cluster" >> hosts
while read IP FQDN HOST SUBNET; do
  ENTRY="${IP} ${FQDN} ${HOST}"
  echo ${ENTRY} >> hosts
done < machines.txt
```

Revisa el archivo `hosts` generado:

```bash
cat hosts
```

DeberÃ­a verse asÃ­ (con tus IPs):

```
# Kubernetes The Hard Way Cluster
192.168.1.10 server.kubernetes.local server
192.168.1.11 node-0.kubernetes.local node-0
192.168.1.12 node-1.kubernetes.local node-1
```

AÃ±ade estas entradas al `/etc/hosts` del `jumpbox`:

```bash
cat hosts >> /etc/hosts
```

Ahora, copia el archivo `hosts` temporal a cada una de las mÃ¡quinas del clÃºster (`server`, `node-0`, `node-1`) y aÃ±ade su contenido a sus respectivos `/etc/hosts`:

```bash
while read IP FQDN HOST SUBNET; do
  scp hosts root@${HOST}:~/
  ssh -n root@${HOST} "cat ~/hosts >> /etc/hosts"
done < machines.txt
```

Verifica que puedes hacer ping por nombre desde el `jumpbox` a las otras mÃ¡quinas y entre ellas. Por ejemplo, desde `jumpbox`:

```bash
ping -c 1 server
ping -c 1 node-0
ping -c 1 node-1
```

Y desde `server`, intenta hacer ping a `node-0`, etc.

âœ… Â¡Listo! Tus mÃ¡quinas ya se pueden comunicar entre sÃ­ por nombre (`server`, `node-0`, `node-1`).

âœ… **Paso 4: Crear CA y certificados TLS** ğŸ”

Esto es uno de los pasos mÃ¡s tÃ©cnicos, pero te lo voy a guiar muy claro.

ğŸ“¦ **Objetivo**

Desde el `jumpbox`:

*   Crear una CA propia.
*   Generar certificados firmados para:
    *   `kube-apiserver`, `kubelet`, `kube-proxy`, etc.
    *   `admin` (usuario de `kubectl`)
*   Distribuir los certificados a las mÃ¡quinas correspondientes.

ğŸ§° **1. AsegÃºrate de estar en el directorio correcto**

En el `jumpbox`:

```bash
cd ~/kubernetes-the-hard-way
```

ğŸ›¡ï¸ **2. Generar la Autoridad Certificadora (CA)**

Primero, genera la clave privada de la CA:

```bash
openssl genrsa -out ca.key 4096
```

Luego, el certificado autofirmado:

```bash
openssl req -x509 -new -sha512 -noenc \
  -key ca.key -days 3653 \
  -config ca.conf \
  -out ca.crt
```

ğŸ“ Esto usarÃ¡ el archivo `ca.conf` ya incluido, que define cÃ³mo debe generarse cada certificado del clÃºster.

ğŸ“„ **3. Generar certificados para todos los componentes**

Ejecuta esto (copia completo):

```bash
certs=(
  "admin" "node-0" "node-1"
  "kube-proxy" "kube-scheduler"
  "kube-controller-manager"
  "kube-api-server"
  "service-accounts"
)

for i in ${certs[*]}; do
  openssl genrsa -out "${i}.key" 4096

  openssl req -new -key "${i}.key" -sha256 \
    -config "ca.conf" -section ${i} \
    -out "${i}.csr"

  openssl x509 -req -days 3653 -in "${i}.csr" \
    -copy_extensions copyall \
    -sha256 -CA "ca.crt" \
    -CAkey "ca.key" \
    -CAcreateserial \
    -out "${i}.crt"
done
```

Verifica que se hayan creado:

```bash
ls -1 *.crt *.key *.csr
```

ğŸ“¤ **4. Copiar certificados a los nodos**

â¤ A los workers (`node-0`, `node-1`):

```bash
for host in node-0 node-1; do
  ssh root@${host} "mkdir -p /var/lib/kubelet/"
  scp ca.crt root@${host}:/var/lib/kubelet/
  scp ${host}.crt root@${host}:/var/lib/kubelet/kubelet.crt
  scp ${host}.key root@${host}:/var/lib/kubelet/kubelet.key
done
```

â¤ Al control plane (`server`):

```bash
scp \
  ca.key ca.crt \
  kube-api-server.key kube-api-server.crt \
  service-accounts.key service-accounts.crt \
  root@server:~/
```

âœ… Â¡Listo! Ya tienes certificados seguros generados y distribuidos.

âœ… **Paso 5: Archivos kubeconfig para autenticaciÃ³n** ğŸ“

Los archivos `kubeconfig` permiten que los componentes de Kubernetes (y usuarios como `admin`) se comuniquen con el `kube-apiserver` de forma segura, usando los certificados TLS que generaste en el paso anterior.

ğŸ§° **Â¿DÃ³nde correr esto?**

Ejecuta todos los comandos de este paso desde el `jumpbox`, dentro del directorio `kubernetes-the-hard-way`.

```bash
cd ~/kubernetes-the-hard-way
```

ğŸ”¹ **1. Generar kubeconfig para los nodos (`node-0` y `node-1`)**

```bash
for host in node-0 node-1; do
  kubectl config set-cluster kubernetes-the-hard-way \
    --certificate-authority=ca.crt \
    --embed-certs=true \
    --server=https://server.kubernetes.local:6443 \
    --kubeconfig=${host}.kubeconfig

  kubectl config set-credentials system:node:${host} \
    --client-certificate=${host}.crt \
    --client-key=${host}.key \
    --embed-certs=true \
    --kubeconfig=${host}.kubeconfig

  kubectl config set-context default \
    --cluster=kubernetes-the-hard-way \
    --user=system:node:${host} \
    --kubeconfig=${host}.kubeconfig

  kubectl config use-context default \
    --kubeconfig=${host}.kubeconfig
done
```

ğŸ”¹ **2. `kube-proxy.kubeconfig`**

```bash
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.crt \
  --embed-certs=true \
  --server=https://server.kubernetes.local:6443 \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-credentials system:kube-proxy \
  --client-certificate=kube-proxy.crt \
  --client-key=kube-proxy.key \
  --embed-certs=true \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=system:kube-proxy \
  --kubeconfig=kube-proxy.kubeconfig

kubectl config use-context default \
  --kubeconfig=kube-proxy.kubeconfig
```

ğŸ”¹ **3. `kube-controller-manager.kubeconfig`**

```bash
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.crt \
  --embed-certs=true \
  --server=https://server.kubernetes.local:6443 \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-credentials system:kube-controller-manager \
  --client-certificate=kube-controller-manager.crt \
  --client-key=kube-controller-manager.key \
  --embed-certs=true \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=system:kube-controller-manager \
  --kubeconfig=kube-controller-manager.kubeconfig

kubectl config use-context default \
  --kubeconfig=kube-controller-manager.kubeconfig
```

ğŸ”¹ **4. `kube-scheduler.kubeconfig`**

```bash
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.crt \
  --embed-certs=true \
  --server=https://server.kubernetes.local:6443 \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-credentials system:kube-scheduler \
  --client-certificate=kube-scheduler.crt \
  --client-key=kube-scheduler.key \
  --embed-certs=true \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=system:kube-scheduler \
  --kubeconfig=kube-scheduler.kubeconfig

kubectl config use-context default \
  --kubeconfig=kube-scheduler.kubeconfig
```

ğŸ”¹ **5. `admin.kubeconfig`**

Este es el que usarÃ¡s tÃº como administrador.

```bash
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.crt \
  --embed-certs=true \
  --server=https://127.0.0.1:6443 \
  --kubeconfig=admin.kubeconfig

kubectl config set-credentials admin \
  --client-certificate=admin.crt \
  --client-key=admin.key \
  --embed-certs=true \
  --kubeconfig=admin.kubeconfig

kubectl config set-context default \
  --cluster=kubernetes-the-hard-way \
  --user=admin \
  --kubeconfig=admin.kubeconfig

kubectl config use-context default \
  --kubeconfig=admin.kubeconfig
```

ğŸ“¤ **6. Distribuir kubeconfig a las mÃ¡quinas**

â¤ A los nodos (`node-0`, `node-1`):

```bash
for host in node-0 node-1; do
  ssh root@${host} "mkdir -p /var/lib/{kube-proxy,kubelet}"

  scp kube-proxy.kubeconfig \
    root@${host}:/var/lib/kube-proxy/kubeconfig

  scp ${host}.kubeconfig \
    root@${host}:/var/lib/kubelet/kubeconfig
done
```

â¤ Al `server`:

```bash
scp admin.kubeconfig \
  kube-controller-manager.kubeconfig \
  kube-scheduler.kubeconfig \
  root@server:~/
```

âœ… Â¡Y listo! Ya tienes todos los `kubeconfig` generados y en su lugar.

âœ… **Paso 6: Crear clave de encriptaciÃ³n para secretos (`encryption-config.yaml`)** ğŸ”

Kubernetes permite encriptar los `Secrets` almacenados en `etcd`. En este paso:

*   Generamos una clave de encriptaciÃ³n segura.
*   Creamos un archivo de configuraciÃ³n para usarla.
*   La copiamos al servidor de control (`server`).

ğŸ§° **Â¿DÃ³nde hacerlo?**

Todo este paso se hace desde el `jumpbox`.

ğŸ”¹ **1. Generar una clave segura**

Ejecuta:

```bash
export ENCRYPTION_KEY=$(head -c 32 /dev/urandom | base64)
```

Puedes verificar que se creÃ³:

```bash
echo $ENCRYPTION_KEY
```

ğŸ”¹ **2. Generar el archivo `encryption-config.yaml`**

El archivo ya estÃ¡ preparado como plantilla en `configs/encryption-config.yaml`, con la variable `${ENCRYPTION_KEY}`.

Usa `envsubst` para reemplazar esa variable:

```bash
envsubst < configs/encryption-config.yaml > encryption-config.yaml
```

Esto generarÃ¡ un archivo real con la clave insertada.

Verifica que estÃ© bien:

```bash
cat encryption-config.yaml
```

DeberÃ­as ver algo asÃ­:

```yaml
kind: EncryptionConfiguration
apiVersion: apiserver.config.k8s.io/v1
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: TU_CLAVE_GENERADA_AQUI
      - identity: {}
```

ğŸ”¹ **3. Copiar a la mÃ¡quina `server`**

```bash
scp encryption-config.yaml root@server:~/
```

âœ… Â¡Enhorabuena! Ya tienes el archivo de configuraciÃ³n de encriptaciÃ³n listo. Este archivo serÃ¡ usado por el `kube-apiserver`.

âœ… **Paso 7: Bootstrapping de etcd** ğŸ“¦

`etcd` es una base de datos distribuida clave-valor que almacena el estado de todo el clÃºster de Kubernetes. AquÃ­ lo desplegaremos en modo de nodo Ãºnico (solo en el `server`).

ğŸ§° **Â¿DÃ³nde hacerlo?**

Este paso se hace dentro del nodo `server`. AsÃ­ que primero conÃ©ctate a Ã©l desde el `jumpbox`:

```bash
ssh root@server
```

Una vez dentro del `server`:

ğŸ”¹ **1. Instalar los binarios de `etcd`**

Desde el `jumpbox`, ya copiaste los binarios necesarios (`etcd`, `etcdctl`) y el archivo de servicio `etcd.service` al directorio home del `server` en pasos anteriores (implÃ­cito en la estructura del tutorial, si no, asegÃºrate de que estÃ©n copiados):

Comandos a ejecutar en el `server`:

```bash
# Mueve los binarios (asumiendo que estÃ¡n en ~/)
mv ~/etcd ~/etcdctl /usr/local/bin/
chmod +x /usr/local/bin/etcd*
```

ğŸ”¹ **2. Configurar `etcd`**

Crea las carpetas necesarias:

```bash
mkdir -p /etc/etcd /var/lib/etcd
chmod 700 /var/lib/etcd
```

Copia los certificados necesarios (ya deben estar en tu home del `server` desde el Paso 4):

```bash
cp ~/ca.crt ~/kube-api-server.crt ~/kube-api-server.key /etc/etcd/
```

ğŸ”¹ **3. Instalar el servicio de `systemd`**

Mueve el archivo de servicio (asumiendo que estÃ¡ en `~/`):

```bash
mv ~/etcd.service /etc/systemd/system/
```

Puedes verificar su contenido si quieres:

```bash
cat /etc/systemd/system/etcd.service
```

ğŸ”¹ **4. Iniciar `etcd`**

Recarga `systemd` y habilita el servicio:

```bash
systemctl daemon-reload
systemctl enable etcd
systemctl start etcd
```

Verifica el estado:

```bash
systemctl status etcd
```

DeberÃ­a mostrar `active (running)`.

ğŸ” **5. Verificar que `etcd` funciona**

Usa `etcdctl` para listar los miembros del clÃºster:

```bash
ETCDCTL_API=3 etcdctl member list \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/etcd/ca.crt \
  --cert=/etc/etcd/kube-api-server.crt \
  --key=/etc/etcd/kube-api-server.key
```
(Nota: El tutorial original usa HTTP para etcd en su configuraciÃ³n de servicio, asÃ­ que el comando serÃ­a mÃ¡s simple si se mantiene esa configuraciÃ³n. El archivo `units/etcd.service` usa `http://127.0.0.1:2379` y `http://127.0.0.1:2380`. Si es asÃ­, el comando de verificaciÃ³n es:)

```bash
etcdctl member list
```

DeberÃ­as ver algo como:

```
ID, STATUS, NAME, PEER ADDRS, CLIENT ADDRS, IS LEARNER
xxxxxxxxxxxxxxxx, started, controller, http://127.0.0.1:2380, http://127.0.0.1:2379, false
```

Sal del `server` para volver al `jumpbox` (`exit`).

âœ… Â¡Listo! `etcd` estÃ¡ funcionando como backend de almacenamiento para Kubernetes.

âœ… **Paso 8: Bootstrapping del Control Plane** ğŸ§ 

Instalaremos en el nodo `server` los tres componentes principales del plano de control:

| Componente                | Rol principal                        |
| :------------------------ | :----------------------------------- |
| `kube-apiserver`          | Punto central de comunicaciÃ³n del clÃºster |
| `kube-controller-manager` | Gestiona controladores internos      |
| `kube-scheduler`          | Asigna pods a nodos disponibles      |

ğŸ§° **Â¿DÃ³nde se hace?**

Todo este paso se ejecuta en el nodo `server`. ConÃ©ctate desde el `jumpbox`:

```bash
ssh root@server
```

Una vez dentro del `server`:

ğŸ”¹ **1. Crear carpeta de configuraciÃ³n**

```bash
mkdir -p /etc/kubernetes/config
```

ğŸ”¹ **2. Instalar los binarios**

Desde el `jumpbox` ya copiaste los binarios y archivos de configuraciÃ³n necesarios al directorio home del `server` en pasos anteriores.

Comandos a ejecutar en el `server`:

```bash
# Mueve los binarios (asumiendo que estÃ¡n en ~/)
mv ~/kube-apiserver ~/kube-controller-manager ~/kube-scheduler ~/kubectl /usr/local/bin/
chmod +x /usr/local/bin/kube*
```

ğŸ”¹ **3. Configurar `kube-apiserver`**

Crear carpeta:

```bash
mkdir -p /var/lib/kubernetes/
```

Mover los certificados y configuraciÃ³n (asumiendo que estÃ¡n en `~/`):

```bash
mv ~/ca.crt ~/ca.key \
   ~/kube-api-server.key ~/kube-api-server.crt \
   ~/service-accounts.key ~/service-accounts.crt \
   ~/encryption-config.yaml \
   /var/lib/kubernetes/
```

Mover archivo de servicio:

```bash
mv ~/kube-apiserver.service /etc/systemd/system/kube-apiserver.service
```

ğŸ”¹ **4. Configurar `kube-controller-manager`**

Mover el `kubeconfig` (asumiendo que estÃ¡ en `~/`):

```bash
mv ~/kube-controller-manager.kubeconfig /var/lib/kubernetes/
```

Mover unit file:

```bash
mv ~/kube-controller-manager.service /etc/systemd/system/
```

ğŸ”¹ **5. Configurar `kube-scheduler`**

Mover el `kubeconfig` (asumiendo que estÃ¡ en `~/`):

```bash
mv ~/kube-scheduler.kubeconfig /var/lib/kubernetes/
```

Mover el archivo YAML de configuraciÃ³n (asumiendo que estÃ¡ en `~/`):

```bash
mv ~/kube-scheduler.yaml /etc/kubernetes/config/
```

Mover unit file:

```bash
mv ~/kube-scheduler.service /etc/systemd/system/
```

ğŸ”¹ **6. Iniciar los servicios del control plane**

```bash
systemctl daemon-reload
systemctl enable kube-apiserver kube-controller-manager kube-scheduler
systemctl start kube-apiserver kube-controller-manager kube-scheduler
```

Verifica que estÃ©n activos (espera unos segundos para que inicien):

```bash
systemctl is-active kube-apiserver
systemctl is-active kube-controller-manager
systemctl is-active kube-scheduler
```

Todos deben devolver `active`. Si no, revisa los logs con `journalctl -u <nombre-del-servicio>`.

ğŸ” **7. Comprobar con `kubectl` (usando el `admin.kubeconfig`)**

```bash
kubectl cluster-info --kubeconfig ~/admin.kubeconfig
```

DeberÃ­as ver algo como:

```
Kubernetes control plane is running at https://127.0.0.1:6443
```

ğŸ” **8. Habilitar permisos del API Server para acceder a `kubelets`**

Aplica la configuraciÃ³n RBAC (asumiendo `kube-apiserver-to-kubelet.yaml` estÃ¡ en `~/`):

```bash
kubectl apply -f ~/kube-apiserver-to-kubelet.yaml \
  --kubeconfig ~/admin.kubeconfig
```

Sal del `server` para volver al `jumpbox` (`exit`).

âœ… Â¡Listo! Tu control plane estÃ¡ instalado y funcionando ğŸš€

âœ… **Paso 9: Bootstrapping de los nodos Worker (`node-0` y `node-1`)** âš™ï¸

En este paso vas a instalar en cada nodo worker:

| Componente      | FunciÃ³n principal                        |
| :-------------- | :--------------------------------------- |
| `containerd`    | Ejecuta contenedores                     |
| `runc`          | Ejecuta contenedores compatibles con OCI |
| `kubelet`       | Agente que corre en cada nodo            |
| `kube-proxy`    | Administra la red de servicios           |
| `CNI Plugins`   | Red entre pods                           |

ğŸ”§ **Â¿CÃ³mo lo haremos?**

*   Desde el `jumpbox` copiarÃ¡s binarios y config a los workers.
*   Luego entrarÃ¡s a cada worker (`node-0` y `node-1`) y ejecutarÃ¡s comandos para instalarlos.

ğŸ§° **Parte 1: Preparar desde el `jumpbox`**

ğŸ”¹ **1. Personaliza y copia configuraciÃ³n de red (`10-bridge.conf` y `kubelet-config.yaml`)**:

```bash
for HOST in node-0 node-1; do
  SUBNET=$(grep ${HOST} machines.txt | cut -d " " -f 4) # AsegÃºrate que machines.txt tenga la 4ta columna con la subred del pod

  # Crea archivos de configuraciÃ³n personalizados para cada nodo
  sed "s|SUBNET|${SUBNET}|g" configs/10-bridge.conf > 10-bridge-${HOST}.conf
  # kubelet-config.yaml no parece tener un placeholder SUBNET en el original, pero si lo tuviera, se harÃ­a igual.
  # Si no hay placeholder, se copia el mismo archivo.
  cp configs/kubelet-config.yaml kubelet-config-${HOST}.yaml

  scp 10-bridge-${HOST}.conf root@${HOST}:~/10-bridge.conf
  scp kubelet-config-${HOST}.yaml root@${HOST}:~/kubelet-config.yaml

  # Limpia los archivos temporales
  rm 10-bridge-${HOST}.conf kubelet-config-${HOST}.yaml
done
```

ğŸ”¹ **2. Copia binarios y unidades `systemd`**:

```bash
for HOST in node-0 node-1; do
  scp \
    downloads/worker/* \
    downloads/client/kubectl \
    configs/99-loopback.conf \
    configs/containerd-config.toml \
    configs/kube-proxy-config.yaml \
    units/containerd.service \
    units/kubelet.service \
    units/kube-proxy.service \
    root@${HOST}:~/
done
```

ğŸ”¹ **3. Copia plugins CNI**:

```bash
for HOST in node-0 node-1; do
  ssh root@${HOST} "mkdir -p ~/cni-plugins/"
  scp downloads/cni-plugins/* root@${HOST}:~/cni-plugins/
done
```

ğŸ§± **Parte 2: En cada nodo worker (`node-0` y luego `node-1`)**

Ahora, entra uno por uno. Primero a `node-0`:

```bash
ssh root@node-0
```

Y ejecuta los siguientes comandos. Luego, sal y repite los mismos comandos en `node-1`.

ğŸ”¹ **1. Instala dependencias del sistema**

```bash
apt-get update
apt-get -y install socat conntrack ipset kmod
```

ğŸ”¹ **2. Desactiva el swap**

```bash
swapoff -a
# Y comenta la lÃ­nea de swap en /etc/fstab para que sea persistente
sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
```

Verifica:

```bash
swapon --show
```
(No deberÃ­a mostrar nada)

ğŸ”¹ **3. Crea los directorios**

```bash
mkdir -p \
  /etc/cni/net.d \
  /opt/cni/bin \
  /var/lib/kubelet \
  /var/lib/kube-proxy \
  /var/lib/kubernetes \
  /var/run/kubernetes
```

ğŸ”¹ **4. Instala los binarios (asumiendo que estÃ¡n en `~/`)**

```bash
mv ~/crictl ~/kube-proxy ~/kubelet ~/runc /usr/local/bin/
mv ~/containerd ~/containerd-shim-runc-v2 ~/containerd-stress /bin/ # Ajustado segÃºn el tutorial
mv ~/cni-plugins/* /opt/cni/bin/
chmod +x /usr/local/bin/* /bin/containerd* /opt/cni/bin/*
```

ğŸ”¹ **5. Configura red (CNI)**

Mueve los archivos de configuraciÃ³n CNI (asumiendo que estÃ¡n en `~/`):

```bash
mv ~/10-bridge.conf ~/99-loopback.conf /etc/cni/net.d/
```

Activa mÃ³dulo de red:

```bash
modprobe br_netfilter # Corregido: guion bajo en lugar de guion
echo "br_netfilter" >> /etc/modules-load.d/modules.conf
```

Habilita `iptables` para el bridge:

```bash
echo "net.bridge.bridge-nf-call-iptables = 1" >> /etc/sysctl.d/kubernetes.conf
echo "net.bridge.bridge-nf-call-ip6tables = 1" >> /etc/sysctl.d/kubernetes.conf
sysctl -p /etc/sysctl.d/kubernetes.conf
```

ğŸ”¹ **6. Configura `containerd`**

```bash
mkdir -p /etc/containerd/
mv ~/containerd-config.toml /etc/containerd/config.toml
mv ~/containerd.service /etc/systemd/system/
```

ğŸ”¹ **7. Configura `kubelet`**

Mueve `kubelet-config.yaml` (asumiendo que estÃ¡ en `~/`) y `kubelet.service`:

```bash
mv ~/kubelet-config.yaml /var/lib/kubelet/
# TambiÃ©n necesitas el kubeconfig del kubelet y el ca.crt que copiaste en el Paso 4 y Paso 5
# Estos ya deberÃ­an estar en /var/lib/kubelet/ (ca.crt, kubelet.crt, kubelet.key) y
# /var/lib/kubelet/kubeconfig
mv ~/kubelet.service /etc/systemd/system/
```

ğŸ”¹ **8. Configura `kube-proxy`**

Mueve `kube-proxy-config.yaml` (asumiendo que estÃ¡ en `~/`) y `kube-proxy.service`:

```bash
mv ~/kube-proxy-config.yaml /var/lib/kube-proxy/
# kube-proxy tambiÃ©n necesita su kubeconfig, ya copiado en /var/lib/kube-proxy/kubeconfig en el Paso 5
mv ~/kube-proxy.service /etc/systemd/system/
```

ğŸ”¹ **9. Inicia los servicios**

```bash
systemctl daemon-reload
systemctl enable containerd kubelet kube-proxy
systemctl start containerd kubelet kube-proxy
```

Verifica:

```bash
systemctl is-active kubelet
systemctl is-active containerd
systemctl is-active kube-proxy
```

Todos deben devolver `active`.

Sal de `node-0` (`exit`) y **repite todos estos pasos de la "Parte 2" en `node-1`**.

ğŸ” **Verifica desde el servidor (`server`)**

Vuelve al `jumpbox` y conÃ©ctate al `server`:

```bash
ssh root@server
```

Luego, usa el `admin.kubeconfig` (que deberÃ­a estar en `~/admin.kubeconfig` en el `server`):

```bash
kubectl get nodes --kubeconfig ~/admin.kubeconfig
```

Debes ver ambos nodos, y despuÃ©s de unos momentos, deberÃ­an pasar a `Ready`:

```
NAME     STATUS   ROLES    AGE   VERSION
node-0   Ready    <none>   1m    v1.32.3
node-1   Ready    <none>   10s   v1.32.3
```
(La edad y el estado pueden variar inicialmente)

Sal del `server` (`exit`).

âœ… Â¡Tus nodos worker ya estÃ¡n funcionando! ğŸ‰

âœ… **Paso 10: Configurar `kubectl` en el `jumpbox` para acceso remoto**

Este paso te permite usar `kubectl` directamente desde el `jumpbox`, sin tener que estar en el nodo `server`.

ğŸ”§ **1. En el `jumpbox`, instala el archivo `kubeconfig` de `admin`**

AsegÃºrate de tener los certificados `admin.crt`, `admin.key`, y `ca.crt` en el directorio actual (`~/kubernetes-the-hard-way`) del `jumpbox`.

Ejecuta (estos comandos crean `~/.kube/config`):

```bash
kubectl config set-cluster kubernetes-the-hard-way \
  --certificate-authority=ca.crt \
  --embed-certs=true \
  --server=https://server.kubernetes.local:6443

kubectl config set-credentials admin \
  --client-certificate=admin.crt \
  --client-key=admin.key

kubectl config set-context kubernetes-the-hard-way \
  --cluster=kubernetes-the-hard-way \
  --user=admin

kubectl config use-context kubernetes-the-hard-way
```

Esto crea un archivo `~/.kube/config` con los datos necesarios para `kubectl`.

ğŸ” **2. Verificar conectividad con el servidor**

```bash
kubectl version
```

DeberÃ­as ver algo como:

```
Client Version: v1.32.3
Server Version: v1.32.3
```

ğŸ” **3. Verificar estado del clÃºster**

```bash
kubectl get nodes
```

Debe devolver:

```
NAME     STATUS   ROLES    AGE   VERSION
node-0   Ready    <none>   ...   v1.32.3
node-1   Ready    <none>   ...   v1.32.3
```

ğŸ‰ Â¡Y listo! Ya puedes administrar tu clÃºster con `kubectl` desde el `jumpbox`.

âœ… **Paso 11: Rutas de red entre Pods** ğŸŒ

Kubernetes asigna a cada nodo un rango de IPs (subred) para sus Pods. Pero, por defecto, no existen rutas entre esas subredes en diferentes mÃ¡quinas, asÃ­ que los Pods no pueden comunicarse entre nodos. Este paso corrige eso aÃ±adiendo rutas manuales.

ğŸ—ºï¸ **Supuestos**

SegÃºn tu `machines.txt` (ejemplo):

*   `node-0`: subred `10.200.0.0/24`, IP `192.168.1.11`
*   `node-1`: subred `10.200.1.0/24`, IP `192.168.1.12`

Ajusta las IPs y subredes a las tuyas.

ğŸ”§ **Paso a paso (ejecutar desde el `jumpbox`)**

ğŸ”¹ **1. AÃ±adir rutas en el `server` para los workers**

```bash
# Variables para IPs y Subredes (ajusta si tu machines.txt es diferente o no estÃ¡ disponible)
NODE_0_IP=$(grep node-0 machines.txt | cut -d " " -f 1)
NODE_0_SUBNET=$(grep node-0 machines.txt | cut -d " " -f 4)
NODE_1_IP=$(grep node-1 machines.txt | cut -d " " -f 1)
NODE_1_SUBNET=$(grep node-1 machines.txt | cut -d " " -f 4)

ssh root@server <<EOF
ip route add ${NODE_0_SUBNET} via ${NODE_0_IP}
ip route add ${NODE_1_SUBNET} via ${NODE_1_IP}
EOF
```

ğŸ”¹ **2. AÃ±adir rutas en `node-0` hacia `node-1`**

```bash
ssh root@node-0 "ip route add ${NODE_1_SUBNET} via ${NODE_1_IP}"
```

ğŸ”¹ **3. AÃ±adir rutas en `node-1` hacia `node-0`**

```bash
ssh root@node-1 "ip route add ${NODE_0_SUBNET} via ${NODE_0_IP}"
```

ğŸ§ª **Verifica las rutas**

Ejemplo en el `server`:

```bash
ssh root@server ip route
```

DeberÃ­as ver algo como (interfaz `ensX` puede variar):

```
...
10.200.0.0/24 via 192.168.1.11 dev ensX
10.200.1.0/24 via 192.168.1.12 dev ensX
...
```
Verifica tambiÃ©n en `node-0` y `node-1`.

âœ… Â¡Listo! Ahora los Pods podrÃ¡n comunicarse entre nodos a travÃ©s de sus IPs internas.

âœ… **Paso 12: Smoke Test** ğŸ”¥

Este es el momento de la verdad: vamos a comprobar que el clÃºster funciona de verdad ejecutando workloads reales.

ğŸ§° **Â¿DÃ³nde ejecutar todo esto?**

Desde el `jumpbox`, usando `kubectl`.

AsegÃºrate de tener configurado el archivo `~/.kube/config` correctamente (lo hiciste en el paso 10).

ğŸ”¹ **1. Crear un Secret y verificar que estÃ© cifrado**

```bash
kubectl create secret generic kubernetes-the-hard-way \
  --from-literal="mykey=mydata"
```

Ahora desde el `server`, verifica que estÃ© cifrado en `etcd`:

```bash
ssh root@server \
  'etcdctl get /registry/secrets/default/kubernetes-the-hard-way | hexdump -C'
```

ğŸ” Busca que la salida comience con algo como:

```
... k8s:enc:aescbc:v1:key1:...
```

âœ… Eso confirma que los Secrets se estÃ¡n almacenando cifrados.

ğŸ”¹ **2. Crear un Deployment (`nginx`)**

```bash
kubectl create deployment nginx --image=nginx:latest
```

Verifica (espera a que el pod estÃ© `Running`):

```bash
kubectl get pods -l app=nginx
```

Debes ver el pod corriendo:

```
NAME                     READY   STATUS    RESTARTS   AGE
nginx-xxxxxxx-xxxxx      1/1     Running   0          Xs
```

ğŸ”¹ **3. Port-forward para probar HTTP**

```bash
POD_NAME=$(kubectl get pods -l app=nginx -o jsonpath="{.items[0].metadata.name}")
kubectl port-forward $POD_NAME 8080:80
```

En **otra terminal** en tu `jumpbox`, ejecuta:

```bash
curl --head http://127.0.0.1:8080
```

âœ… DeberÃ­as recibir un `HTTP/1.1 200 OK`.

Vuelve a la terminal del `port-forward` y presiona `Ctrl+C` para detenerlo.

ğŸ”¹ **4. Leer logs del pod**

```bash
kubectl logs $POD_NAME
```

VerÃ¡s logs de acceso, por ejemplo, del `curl`.

ğŸ”¹ **5. Ejecutar un comando dentro del contenedor**

```bash
kubectl exec -ti $POD_NAME -- nginx -v
```

Debe mostrar la versiÃ³n de `nginx`, por ejemplo:

```
nginx version: nginx/1.2x.x
```

ğŸ”¹ **6. Exponer el servicio vÃ­a `NodePort`**

```bash
kubectl expose deployment nginx \
  --port 80 \
  --type NodePort
```

Verifica el puerto asignado:

```bash
NODE_PORT=$(kubectl get svc nginx \
  --output=jsonpath='{.spec.ports[0].nodePort}')
echo "NodePort asignado: $NODE_PORT"
```

Ahora identifica en quÃ© nodo estÃ¡ corriendo el pod:

```bash
NODE_NAME=$(kubectl get pods -l app=nginx \
  -o jsonpath="{.items[0].spec.nodeName}")
echo "nginx estÃ¡ corriendo en: $NODE_NAME (IP: $(grep $NODE_NAME machines.txt | cut -d ' ' -f1))"
# Necesitas la IP del nodo, no el nombre para el curl.
NODE_IP=$(grep $NODE_NAME machines.txt | cut -d ' ' -f1)
```

Haz la prueba accediendo al servicio usando la IP de uno de tus nodos (`node-0` o `node-1`) y el `NodePort`:

```bash
curl -I http://${NODE_IP}:${NODE_PORT}
```

âœ… DeberÃ­as obtener otro `HTTP/1.1 200 OK`.

ğŸ‰ Â¡ClÃºster funcionando correctamente!

âœ… **Paso 13: Limpieza del clÃºster** ğŸ§¹

Este paso es opcional, pero Ãºtil si ya terminaste tus pruebas o quieres liberar recursos.

ğŸ”¹ **Â¿QuÃ© se limpia?**

*   Las instancias de mÃ¡quinas del `server`, `node-0`, `node-1` (no el `jumpbox`, si quieres repetir).
*   No se requiere eliminar configuraciones a mano â€” basta con borrar las VMs.

ğŸ§¨ **CÃ³mo hacerlo**

Si estÃ¡s en un entorno local o virtualizado (ej. Proxmox, VirtualBox, VMware):

ğŸ”» Simplemente elimina las 3 mÃ¡quinas virtuales:

*   `server`
*   `node-0`
*   `node-1`

Si estÃ¡s en la nube (AWS, GCP, etc.), destruye los recursos (VMs, discos, redes) que creaste manualmente.

ğŸ§¼ **Opcional: Limpia el entorno del `jumpbox`**

Si tambiÃ©n quieres limpiar el `jumpbox`, puedes borrar el directorio de trabajo:

```bash
rm -rf ~/kubernetes-the-hard-way
```
Y el `~/.kube/config`:
```bash
rm -f ~/.kube/config
```

âœ… Â¡Todo limpio!

ğŸ“ **Â¡Felicidades!**

Has completado Kubernetes The Hard Way, instalando todo a mano:

*   Certificados TLS
*   `etcd`
*   Control Plane (`kube-apiserver`, `kube-controller-manager`, `kube-scheduler`)
*   Workers (`kubelet`, `kube-proxy`, `containerd`)
*   Redes CNI
*   Acceso RBAC
*   `kubectl` desde el `jumpbox`
*   Smoke test con Ã©xito

ğŸš€ Entiendes lo que pasa â€œbajo el capÃ³â€ de Kubernetes. Muchos usan `kubeadm` o `k3s`, pero tÃº ahora sabes cÃ³mo funciona de verdad.

---

**AutomatizaciÃ³n de la InstalaciÃ³n**

Ahora que has completado la instalaciÃ³n manual de Kubernetes siguiendo el enfoque de "The Hard Way", es natural buscar formas de automatizar este proceso para futuras implementaciones. Existen varias guÃ­as y proyectos que replican este enfoque utilizando herramientas como Ansible y Terraform, manteniendo la filosofÃ­a de comprender cada componente del clÃºster.

ğŸ› ï¸ **Opciones para Automatizar la InstalaciÃ³n de Kubernetes**

1.  **AutomatizaciÃ³n con Ansible: `kubernetes-hard-way-ansible`**

    Este proyecto de `zufardhiyaulhaq` automatiza la instalaciÃ³n de Kubernetes siguiendo el enfoque de "The Hard Way" utilizando Ansible. Incluye soporte para Vagrant y OpenID Connect (OIDC), y proporciona funcionalidades para renovar certificados, aÃ±adir nuevos nodos worker y actualizar la versiÃ³n de Kubernetes.

    *   **CaracterÃ­sticas principales**:
        *   AutomatizaciÃ³n completa de la instalaciÃ³n de Kubernetes (versiÃ³n especÃ­fica del proyecto).
        *   Soporte para Flannel, CNI, `containerd`, `runc` y otros componentes esenciales.
        *   Playbooks para tareas comunes como renovaciÃ³n de certificados y adiciÃ³n de nodos.
    *   **Pasos generales**:
        *   Preparar el entorno de Ansible en el nodo de despliegue.
        *   Configurar el inventario de hosts y variables de grupo.
        *   Ejecutar el playbook principal para desplegar el clÃºster.
    *   **Repositorio**: [kubernetes-hard-way-ansible en GitHub](https://github.com/zufardhiyaulhaq/kubernetes-hard-way-ansible)

2.  **AutomatizaciÃ³n con Terraform y Ansible en AWS**

    El equipo de OpenCredo ha desarrollado una guÃ­a detallada para desplegar un clÃºster de Kubernetes en AWS utilizando Terraform para la provisiÃ³n de infraestructura y Ansible para la configuraciÃ³n del clÃºster. Este enfoque automatiza los pasos de "The Hard Way" adaptÃ¡ndolos a un entorno en la nube.

    *   **CaracterÃ­sticas principales**:
        *   ProvisiÃ³n de infraestructura en AWS con Terraform (VPC, subredes, instancias EC2).
        *   ConfiguraciÃ³n de Kubernetes con Ansible, incluyendo `etcd`, control plane y nodos worker.
        *   Despliegue de un servicio de ejemplo (`nginx`) para verificar el funcionamiento del clÃºster.
    *   **Pasos generales**:
        *   Utilizar Terraform para crear la infraestructura necesaria en AWS.
        *   Aplicar playbooks de Ansible para instalar y configurar Kubernetes en las instancias creadas.
    *   **GuÃ­a detallada**: [Kubernetes from scratch to AWS with Terraform and Ansible (part 1)](https://medium.com/@opencredo/kubernetes-from-scratch-to-aws-with-terraform-and-ansible-part-1-a7549f3a8a0f)
    *   **Repositorio**: [k8s-terraform-ansible-sample en GitHub](https://github.com/opencredo/k8s-terraform-ansible-sample)

3.  **AutomatizaciÃ³n Local con Terraform y Ansible**

    Si prefieres un entorno local, este tutorial de Kraven Security muestra cÃ³mo desplegar un clÃºster de Kubernetes utilizando Terraform y Ansible en un entorno local, como Proxmox. Este enfoque es ideal para laboratorios y pruebas en entornos controlados.

    *   **CaracterÃ­sticas principales**:
        *   ProvisiÃ³n de mÃ¡quinas virtuales locales con Terraform.
        *   ConfiguraciÃ³n de Kubernetes con Ansible, a menudo usando `kubeadm` para simplificar ciertas partes pero manteniendo el control sobre la infraestructura.
        *   Despliegue de aplicaciones de ejemplo para verificar el funcionamiento del clÃºster.
    *   **Pasos generales**:
        *   Utilizar Terraform para crear las mÃ¡quinas virtuales necesarias.
        *   Aplicar playbooks de Ansible para instalar y configurar Kubernetes en las VMs.
        *   Desplegar aplicaciones de prueba y verificar la conectividad.
    *   **GuÃ­a detallada**: [How To Create A Local Kubernetes Cluster: Terraform And Ansible](https://kravensecurity.com/how-to-create-a-local-kubernetes-cluster-terraform-and-ansible/)

ğŸ” **Comparativa de Enfoques**

| Proyecto                                 | Herramientas        | Entorno       | CaracterÃ­sticas destacadas                                     |
| :--------------------------------------- | :------------------ | :------------ | :------------------------------------------------------------- |
| `kubernetes-hard-way-ansible`            | Ansible             | Local/Vagrant | AutomatizaciÃ³n completa siguiendo "The Hard Way"               |
| OpenCredo: Terraform + Ansible en AWS    | Terraform, Ansible  | AWS           | ProvisiÃ³n y configuraciÃ³n automatizada en la nube             |
| Kraven Security: Terraform + Ansible local | Terraform, Ansible  | Local         | Despliegue local ideal para laboratorios y pruebas (puede usar `kubeadm`) |

âœ… **RecomendaciÃ³n**

Dado que ya has realizado la instalaciÃ³n manual y estÃ¡s familiarizado con los componentes de Kubernetes, te recomiendo explorar el proyecto `kubernetes-hard-way-ansible`. Este proyecto automatiza el proceso que ya conoces, permitiÃ©ndote comparar cada paso y entender cÃ³mo se traduce en tareas de Ansible.

---

**Manual TeÃ³rico de "Kubernetes The Hard Way": Comprendiendo los Cimientos**

**IntroducciÃ³n: Â¿Por QuÃ© "The Hard Way" y QuÃ© Aprenderemos?**

"Kubernetes The Hard Way" no es la forma mÃ¡s rÃ¡pida de tener un clÃºster funcional, Â¡pero sÃ­ una de las mÃ¡s enriquecedoras! Al construir cada componente manualmente, desmitificamos la "magia" de Kubernetes. Este manual teÃ³rico te acompaÃ±arÃ¡ en ese viaje, explicando los conceptos detrÃ¡s de cada acciÃ³n prÃ¡ctica.

**Objetivo de este Manual TeÃ³rico:**

1.  **Entender la Arquitectura de Kubernetes:** Visualizar cÃ³mo los componentes interactÃºan.
2.  **Comprender la Seguridad:** Por quÃ© los certificados y la encriptaciÃ³n son cruciales.
3.  **Asimilar el Flujo de Trabajo:** Desde la solicitud de un Pod hasta su ejecuciÃ³n.
4.  **Conocer los Primitivos de Red:** CÃ³mo se comunican los Pods y Servicios.
5.  **Valorar la AutomatizaciÃ³n:** Entender quÃ© problemas resuelven herramientas como `kubeadm` despuÃ©s de haberlo hecho a mano.

---

**Parte I: PreparaciÃ³n y Cimientos de la Infraestructura**

**CapÃ­tulo 1: Las MÃ¡quinas â€“ El Lienzo de Nuestro ClÃºster (Corresponde al Paso 1 PrÃ¡ctico)**

*   **Â¿Por quÃ© mÃ¡quinas dedicadas (virtuales o fÃ­sicas)?**
    Kubernetes es un sistema distribuido. Necesita mÃºltiples "ordenadores" (nodos) para funcionar.
    *   **Plano de Control (`server`):** Es el cerebro. AquÃ­ residen los componentes que toman decisiones globales sobre el clÃºster (ej. dÃ³nde ejecutar una aplicaciÃ³n, cÃ³mo mantener el nÃºmero deseado de rÃ©plicas). Separarlo de los nodos de trabajo es una prÃ¡ctica estÃ¡ndar para la estabilidad y seguridad.
    *   **Nodos de Trabajo (`node-0`, `node-1`):** Son los mÃºsculos. AquÃ­ es donde tus aplicaciones (contenedores dentro de Pods) realmente se ejecutan. Necesitamos al menos uno, pero dos o mÃ¡s nos permiten ver cÃ³mo Kubernetes distribuye la carga y maneja fallos.
    *   **Jumpbox:** Es nuestra "mesa de operaciones". Centraliza las herramientas, binarios y configuraciones. Evita instalar todo en cada mÃ¡quina del clÃºster o en tu mÃ¡quina personal, manteniendo el entorno limpio y reproducible.
*   **Â¿Por quÃ© Debian 12 (Bookworm)?**
    Es una distribuciÃ³n de Linux estable, popular y con buen soporte comunitario. PodrÃ­amos usar otras (CentOS, Ubuntu), pero para un tutorial, la consistencia es clave. Lo importante es un kernel de Linux moderno y herramientas estÃ¡ndar.
*   **Â¿Por quÃ© acceso `root`?**
    Para este tutorial, usamos `root` por conveniencia, ya que instalaremos software a nivel de sistema, modificaremos archivos de configuraciÃ³n crÃ­ticos y gestionaremos servicios. **En un entorno de producciÃ³n, NUNCA se trabajarÃ­a directamente como `root` para todo.** Se usarÃ­a `sudo` con permisos especÃ­ficos o herramientas de gestiÃ³n de configuraciÃ³n que operan con privilegios elevados de forma controlada.

**CapÃ­tulo 2: El Jumpbox â€“ Nuestro Centro de Comando (Corresponde al Paso 2 PrÃ¡ctico)**

*   **Herramientas BÃ¡sicas (`wget`, `curl`, `vim`, `openssl`, `git`):**
    *   `wget`/`curl`: Para descargar archivos de internet (los binarios de Kubernetes).
    *   `vim`: Un editor de texto para modificar archivos de configuraciÃ³n (puedes usar `nano` u otro).
    *   `openssl`: Herramienta fundamental para crear y gestionar certificados TLS/SSL, la base de la comunicaciÃ³n segura.
    *   `git`: Para clonar el repositorio de "Kubernetes The Hard Way", que contiene plantillas y la estructura del tutorial.
*   **Descarga Centralizada de Binarios:**
    *   **Â¿QuÃ© son estos binarios?** Son los programas ejecutables que componen Kubernetes:
        *   `etcd`: La base de datos del clÃºster.
        *   `kube-apiserver`: El frontal de la API, el punto de entrada principal.
        *   `kube-controller-manager`: Vigila el estado y ejecuta bucles de reconciliaciÃ³n.
        *   `kube-scheduler`: Decide en quÃ© nodo se ejecuta un Pod.
        *   `kubelet`: Agente en cada nodo worker, gestiona los Pods en ese nodo.
        *   `kube-proxy`: Gestiona las reglas de red en cada nodo para los Servicios.
        *   `kubectl`: La herramienta de lÃ­nea de comandos para interactuar con el clÃºster.
        *   `containerd`, `runc`, `crictl`: Componentes del runtime de contenedores.
        *   Plugins CNI: Para la red de los Pods.
    *   **Â¿Por quÃ© descargarlos en el Jumpbox?**
        1.  **Consistencia de VersiÃ³n:** Asegura que todas las mÃ¡quinas del clÃºster usen exactamente la misma versiÃ³n de cada componente. Esto es vital para evitar incompatibilidades.
        2.  **Eficiencia:** Se descargan una sola vez, ahorrando ancho de banda y tiempo.
*   **Versiones EspecÃ­ficas:** Kubernetes evoluciona rÃ¡pidamente. Un tutorial como este fija versiones especÃ­ficas para garantizar que los pasos sean reproducibles. Las APIs y comportamientos pueden cambiar entre versiones.

**CapÃ­tulo 3: Provisionamiento de Nodos â€“ Identidad y Conectividad (Corresponde al Paso 3 PrÃ¡ctico)**

*   **Archivo `machines.txt`:**
    ActÃºa como una mini-base de datos para nuestro clÃºster. Almacena informaciÃ³n crucial (IP, nombre de host, FQDN, subred de Pods) que usaremos repetidamente en scripts y configuraciones.
*   **SSH Keys (Claves SSH):**
    *   **Â¿Por quÃ©?** Para una comunicaciÃ³n segura y automatizada entre el `jumpbox` y los nodos del clÃºster. En lugar de escribir contraseÃ±as cada vez, usamos un par de claves criptogrÃ¡ficas (pÃºblica y privada). La clave pÃºblica se instala en los nodos, y solo el `jumpbox` (que tiene la clave privada) puede autenticarse.
    *   Es la base para que herramientas de automatizaciÃ³n (como Ansible, que veremos al final) puedan gestionar mÃºltiples servidores.
*   **Hostnames (Nombres de Host):**
    *   **Â¿Por quÃ©?** Los humanos (y los sistemas) prefieren nombres a direcciones IP. `server.kubernetes.local` es mÃ¡s fÃ¡cil de recordar y gestionar que `192.168.1.10`.
    *   Muchos componentes de Kubernetes se referenciarÃ¡n entre sÃ­ usando estos nombres. Los certificados TLS tambiÃ©n validarÃ¡n estos nombres.
*   **`/etc/hosts`:**
    *   **Â¿QuÃ© es?** Un archivo local que mapea nombres de host a direcciones IP. ActÃºa como un mini-DNS local.
    *   **Â¿Por quÃ© modificarlo en TODAS las mÃ¡quinas?** Cada mÃ¡quina del clÃºster (incluido el `jumpbox`) necesita poder resolver los nombres de las otras mÃ¡quinas. Si `node-0` necesita hablar con `server.kubernetes.local`, debe saber quÃ© IP corresponde a ese nombre.
    *   **Alternativa en ProducciÃ³n:** En entornos mÃ¡s grandes o de producciÃ³n, se usarÃ­a un servidor DNS centralizado en lugar de modificar `/etc/hosts` en cada mÃ¡quina. Para nuestro tutorial, `/etc/hosts` es mÃ¡s simple.

---

**Parte II: Seguridad â€“ La Columna Vertebral del ClÃºster**

**CapÃ­tulo 4: Autoridad Certificadora (CA) y Certificados TLS (Corresponde al Paso 4 PrÃ¡ctico)**

*   **Â¿QuÃ© es TLS/SSL?**
    Transport Layer Security (TLS) â€“sucesor de Secure Sockets Layer (SSL)â€“ es un protocolo criptogrÃ¡fico que proporciona comunicaciones seguras a travÃ©s de una red. Ofrece:
    1.  **AutenticaciÃ³n:** Verifica la identidad de las partes que se comunican (Â¿realmente estoy hablando con el `kube-apiserver`?).
    2.  **EncriptaciÃ³n:** Cifra los datos transmitidos para que no puedan ser leÃ­dos por terceros.
    3.  **Integridad:** Asegura que los datos no hayan sido manipulados durante la transmisiÃ³n.
*   **Â¿Por quÃ© Kubernetes necesita TLS?**
    Todos los componentes de Kubernetes se comunican a travÃ©s de la red (incluso si estÃ¡n en la misma mÃ¡quina, a travÃ©s de `localhost`). Esta comunicaciÃ³n incluye informaciÃ³n sensible (configuraciones, secretos, Ã³rdenes). Sin TLS, esta comunicaciÃ³n serÃ­a vulnerable a escuchas (eavesdropping) y ataques de "hombre en el medio" (man-in-the-middle).
*   **Autoridad Certificadora (CA):**
    *   **Â¿QuÃ© es?** Es una entidad de confianza que emite y firma certificados digitales. Un certificado digital vincula una identidad (como `kube-apiserver`) a una clave pÃºblica.
    *   **Nuestra CA autofirmada:** En este tutorial, creamos nuestra propia CA. Esto significa que nosotros somos la raÃ­z de confianza. Todos los componentes del clÃºster confiarÃ¡n en los certificados emitidos por *nuestra* CA.
    *   **ProducciÃ³n:** En producciÃ³n, podrÃ­as usar una CA interna de tu organizaciÃ³n o incluso certificados de CAs pÃºblicas para componentes expuestos a internet (aunque esto es menos comÃºn para la comunicaciÃ³n interna del clÃºster).
*   **Certificados para cada Componente:**
    Cada componente principal (`kube-apiserver`, `kubelet`, `etcd`, `kube-proxy`, `kube-controller-manager`, `kube-scheduler`) y el usuario `admin` obtienen su propio par de clave privada y certificado firmado por nuestra CA.
    *   **Identidad Ãšnica:** Esto les da una identidad Ãºnica.
    *   **AutenticaciÃ³n Mutua (mTLS):** A menudo, no solo el cliente verifica al servidor, sino que el servidor tambiÃ©n verifica al cliente. Por ejemplo, el `kube-apiserver` necesita saber que estÃ¡ hablando con un `kubelet` legÃ­timo, y el `kubelet` necesita saber que estÃ¡ hablando con el `kube-apiserver` legÃ­timo.
*   **`ca.conf` â€“ El Molde de los Certificados:**
    Este archivo de configuraciÃ³n de OpenSSL define las propiedades de cada certificado:
    *   `CN (Common Name)`: El nombre principal del certificado (ej. `kube-apiserver` o `system:node:node-0`). Es crucial para la identificaciÃ³n y, en algunos casos (como los `kubelets`), para la autorizaciÃ³n RBAC.
    *   `O (Organization)`: Usado para agrupar entidades. En Kubernetes, `O=system:masters` otorga privilegios de administrador de clÃºster, y `O=system:nodes` es para los `kubelets`.
    *   `SAN (Subject Alternative Name)`: Permite especificar mÃºltiples nombres de host y direcciones IP para los cuales el certificado es vÃ¡lido. Esto es vital, ya que el `kube-apiserver`, por ejemplo, puede ser accedido por `127.0.0.1`, su IP de red, `kubernetes.default.svc.cluster.local`, etc.
*   **DistribuciÃ³n de Certificados:**
    Cada componente necesita acceso a:
    1.  Su propia clave privada (`componente.key`).
    2.  Su propio certificado (`componente.crt`).
    3.  El certificado de la CA (`ca.crt`) para poder verificar los certificados de otros componentes.
    Las claves privadas deben mantenerse seguras y solo accesibles por el componente que las usa.

**CapÃ­tulo 5: Archivos `kubeconfig` â€“ Las Llaves de Acceso (Corresponde al Paso 5 PrÃ¡ctico)**

*   **Â¿QuÃ© es un archivo `kubeconfig`?**
    Es un archivo YAML que contiene la informaciÃ³n necesaria para que un cliente (como `kubectl` o un componente de Kubernetes) se conecte y autentique con un clÃºster de Kubernetes, especÃ­ficamente con su `kube-apiserver`.
*   **Componentes Clave de un `kubeconfig`:**
    1.  **Clusters:** Define los clÃºsteres disponibles. Cada clÃºster tiene:
        *   `server`: La URL del `kube-apiserver`.
        *   `certificate-authority-data`: El certificado de la CA del clÃºster (embebido y codificado en base64) para que el cliente pueda verificar el certificado del `kube-apiserver`.
    2.  **Users:** Define las identidades de usuario. Cada usuario tiene:
        *   `client-certificate-data`: El certificado del cliente (embebido, base64).
        *   `client-key-data`: La clave privada del cliente (embebida, base64).
        *   (Alternativamente, podrÃ­a usar tokens).
    3.  **Contexts:** Vincula un `user` con un `cluster` (y opcionalmente un `namespace` por defecto). Es la "conexiÃ³n activa".
    4.  `current-context`: Especifica quÃ© contexto usar por defecto.
*   **Â¿Por quÃ© un `kubeconfig` para cada componente?**
    *   **`kubelet`:** Necesita un `kubeconfig` para registrarse con el `kube-apiserver`, enviar el estado del nodo y de los Pods, y obtener las especificaciones de los Pods que debe ejecutar. Su identidad (definida por su certificado `system:node:<nombre-nodo>`) es usada por el Node Authorizer y RBAC.
    *   **`kube-proxy`:** Necesita un `kubeconfig` para obtener informaciÃ³n sobre Servicios y Endpoints del `kube-apiserver` y asÃ­ poder configurar las reglas de red (`iptables`).
    *   **`kube-controller-manager` y `kube-scheduler`:** Necesitan `kubeconfigs` para interactuar con el `kube-apiserver`, observar el estado del clÃºster y realizar cambios (crear/actualizar objetos).
    *   **`admin`:** El `kubeconfig` para el usuario administrador, permitiÃ©ndole usar `kubectl` para gestionar el clÃºster. La URL del servidor aquÃ­ es `https://127.0.0.1:6443` porque este `kubeconfig` especÃ­fico se usa *dentro* del nodo `server`.
*   **`embed-certs=true`:**
    Hace que el `kubeconfig` sea autocontenido al incrustar los datos de los certificados directamente en el archivo, en lugar de referenciar archivos externos. Esto facilita su distribuciÃ³n.

**CapÃ­tulo 6: EncriptaciÃ³n de Secretos en Reposo (Corresponde al Paso 6 PrÃ¡ctico)**

*   **Â¿QuÃ© son los `Secrets` de Kubernetes?**
    Son objetos de Kubernetes diseÃ±ados para almacenar pequeÃ±as cantidades de datos sensibles, como contraseÃ±as, tokens OAuth o claves SSH.
*   **Â¿Por quÃ© encriptarlos "en reposo"?**
    "En reposo" significa que los datos estÃ¡n encriptados mientras estÃ¡n almacenados en la base de datos persistente del clÃºster, que es `etcd`. Si un atacante obtuviera acceso directo a los archivos de `etcd` (por ejemplo, a una copia de seguridad), los `Secrets` no estarÃ­an en texto plano. Es una capa adicional de seguridad (defensa en profundidad).
*   **Â¿CÃ³mo funciona?**
    1.  Cuando creas un `Secret` a travÃ©s del `kube-apiserver`.
    2.  El `kube-apiserver`, antes de escribirlo en `etcd`, lo encripta usando una clave y un proveedor de encriptaciÃ³n configurados.
    3.  Cuando se lee un `Secret`, el `kube-apiserver` lo recupera de `etcd` (donde estÃ¡ encriptado) y lo desencripta antes de entregarlo al cliente que lo solicitÃ³ (si estÃ¡ autorizado).
*   **`encryption-config.yaml`:**
    Este archivo le dice al `kube-apiserver` cÃ³mo encriptar los datos:
    *   `resources`: Especifica quÃ© tipos de objetos encriptar (en nuestro caso, `secrets`).
    *   `providers`: Define una lista ordenada de proveedores de encriptaciÃ³n.
        *   `aescbc`: Utiliza el cifrado AES en modo CBC. Es un algoritmo simÃ©trico fuerte.
            *   `keys`: Una lista de claves. `key1` es solo un nombre. El `secret` es la clave de encriptaciÃ³n real (generada aleatoriamente y codificada en base64). Se pueden listar mÃºltiples claves para la rotaciÃ³n de claves. La primera clave de la lista se usa para encriptar. Todas las claves se pueden usar para desencriptar.
        *   `identity: {}`: Este proveedor simplemente almacena los datos tal cual (sin encriptaciÃ³n). Se incluye como el Ãºltimo de la lista para permitir la lectura de datos que podrÃ­an haber sido escritos antes de que la encriptaciÃ³n estuviera habilitada o con una clave diferente que ya no estÃ¡.
*   **Variable de Entorno `ENCRYPTION_KEY`:**
    Se usa para generar dinÃ¡micamente el archivo `encryption-config.yaml` con una clave Ãºnica cada vez que se ejecuta el tutorial. En un sistema real, esta clave se generarÃ­a y gestionarÃ­a de forma segura.

---

**Parte III: El CorazÃ³n de Kubernetes â€“ El Plano de Control y los Nodos de Trabajo**

**CapÃ­tulo 7: `etcd` â€“ La Fuente Ãšnica de Verdad (Corresponde al Paso 7 PrÃ¡ctico)**

*   **Â¿QuÃ© es `etcd`?**
    Es un almacÃ©n de datos clave-valor distribuido, consistente y altamente disponible. Kubernetes lo utiliza como su base de datos principal para almacenar *todo* el estado del clÃºster:
    *   Configuraciones de nodos, Pods, Servicios, Deployments, Secrets, ConfigMaps, etc.
    *   Estado actual de esos objetos.
    *   Eventos del clÃºster.
*   **Â¿Por quÃ© es tan importante?**
    Es la "fuente Ãºnica de verdad". Todos los demÃ¡s componentes de Kubernetes son (mayoritariamente) sin estado; leen de `etcd` para conocer el estado deseado y actual, y escriben en `etcd` para actualizar el estado. Si `etcd` se pierde y no hay copia de seguridad, todo el estado del clÃºster se pierde.
*   **ConfiguraciÃ³n de un solo nodo para el tutorial:**
    Por simplicidad, configuramos `etcd` en un solo nodo (`server`).
    *   **ProducciÃ³n:** `etcd` se ejecutarÃ­a como un clÃºster de 3 o 5 nodos (un nÃºmero impar para el consenso Raft) para alta disponibilidad y tolerancia a fallos.
*   **ParÃ¡metros Clave de `etcd.service`:**
    *   `--name controller`: Nombre Ãºnico de este miembro de `etcd` en el clÃºster.
    *   `--initial-advertise-peer-urls http://127.0.0.1:2380`: URL que este miembro anuncia a otros miembros para la comunicaciÃ³n entre pares (peer communication).
    *   `--listen-peer-urls http://127.0.0.1:2380`: URLs en las que escucha trÃ¡fico de otros miembros.
    *   `--listen-client-urls http://127.0.0.1:2379`: URLs en las que escucha trÃ¡fico de clientes (como el `kube-apiserver`).
    *   `--advertise-client-urls http://127.0.0.1:2379`: URL que este miembro anuncia a los clientes.
    *   `--initial-cluster controller=http://127.0.0.1:2380`: Define los miembros iniciales del clÃºster.
    *   `--data-dir=/var/lib/etcd`: Directorio donde `etcd` almacena sus datos.
*   **Seguridad de `etcd`:**
    El `etcd.service` proporcionado usa HTTP. En producciÃ³n, se configurarÃ­a TLS para la comunicaciÃ³n con `etcd` (tanto cliente-servidor como servidor-servidor), usando certificados para autenticar al `kube-apiserver` como cliente y para que los miembros de `etcd` se autentiquen entre sÃ­.

**CapÃ­tulo 8: El Plano de Control â€“ El Cerebro del ClÃºster (Corresponde al Paso 8 PrÃ¡ctico)**

El plano de control estÃ¡ compuesto por varios componentes que se ejecutan en el nodo `server`.

1.  **`kube-apiserver`:**
    *   **Rol:** Es el componente central y el Ãºnico con el que los usuarios y otros componentes interactÃºan directamente. ActÃºa como un frontend para el clÃºster.
        *   Expone la API REST de Kubernetes.
        *   Valida y procesa las solicitudes de API (ej. `kubectl create pod ...`).
        *   Persiste el estado de los objetos en `etcd`.
        *   Orquesta la comunicaciÃ³n entre componentes.
    *   **ParÃ¡metros Clave:**
        *   `--etcd-servers=http://127.0.0.1:2379`: Le dice dÃ³nde encontrar `etcd`.
        *   `--client-ca-file=/var/lib/kubernetes/ca.crt`: CA para verificar los certificados de los clientes que se conectan (ej. `kubelet`, `kubectl`).
        *   `--tls-cert-file`, `--tls-private-key-file`: Certificado y clave del propio `kube-apiserver` para servir HTTPS.
        *   `--kubelet-certificate-authority`, `--kubelet-client-certificate`, `--kubelet-client-key`: Para que el `apiserver` actÃºe como cliente y se conecte de forma segura a los `kubelets` (para logs, exec, etc.).
        *   `--service-account-key-file`, `--service-account-signing-key-file`, `--service-account-issuer`: Para gestionar los tokens de las [ServiceAccounts](https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/) (identidades para los Pods).
        *   `--authorization-mode=Node,RBAC`:
            *   `Node`: Un autorizador especial para los `kubelets`.
            *   `RBAC (Role-Based Access Control)`: El mecanismo principal para controlar quiÃ©n puede hacer quÃ© en el clÃºster.
        *   `--encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml`: Apunta al archivo de configuraciÃ³n de encriptaciÃ³n de `Secrets`.
        *   `--bind-address=0.0.0.0`: Escucha en todas las interfaces de red, no solo `localhost`.
        *   `--allow-privileged=true`: Permite la ejecuciÃ³n de contenedores privilegiados (generalmente se necesita para algunos componentes de sistema o drivers).

2.  **`kube-controller-manager`:**
    *   **Rol:** Ejecuta varios "controladores" en segundo plano. Un controlador es un bucle que observa el estado del clÃºster a travÃ©s del `kube-apiserver` y realiza cambios para intentar que el estado actual coincida con el estado deseado.
    *   **Ejemplos de Controladores:** Node controller (maneja nodos caÃ­dos), Replication controller (mantiene el nÃºmero correcto de Pods para un ReplicaSet), Endpoint controller (popula los Endpoints para los Servicios), Service Account & Token controllers, etc.
    *   **ParÃ¡metros Clave:**
        *   `--kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig`: Le dice cÃ³mo conectarse y autenticarse con el `kube-apiserver`.
        *   `--cluster-signing-cert-file`, `--cluster-signing-key-file`: CA usada para firmar ciertos certificados generados por el clÃºster (como los certificados de los `kubelets` si se usa la aprobaciÃ³n CSR).
        *   `--root-ca-file=/var/lib/kubernetes/ca.crt`: CA raÃ­z para verificar el `kube-apiserver`.
        *   `--service-account-private-key-file`: Clave privada para firmar tokens de ServiceAccount.
        *   `--cluster-cidr`, `--service-cluster-ip-range`: Rangos de IP para Pods y Servicios, respectivamente.

3.  **`kube-scheduler`:**
    *   **Rol:** Su Ãºnica tarea es observar los Pods reciÃ©n creados que aÃºn no tienen un nodo asignado (`nodeName` vacÃ­o) y decidir en quÃ© nodo deben ejecutarse.
    *   **Proceso de Scheduling:** Considera muchos factores: requerimientos de recursos del Pod (CPU, memoria), afinidad/anti-afinidad de nodos y Pods, taints y tolerations, disponibilidad de volÃºmenes, etc.
    *   **ParÃ¡metros Clave:**
        *   `--config=/etc/kubernetes/config/kube-scheduler.yaml`: Apunta a su archivo de configuraciÃ³n, que a su vez contiene la ruta al `kubeconfig`.
*   **`systemd` Services:**
    Usamos `systemd` para gestionar estos componentes como servicios del sistema. Esto asegura que se inicien automÃ¡ticamente al arrancar la mÃ¡quina y se reinicien si fallan.
*   **RBAC para `kube-apiserver` a `kubelet` (`kube-apiserver-to-kubelet.yaml`):**
    El `kube-apiserver` a veces necesita actuar como cliente y conectarse a la API de los `kubelets` (ej. para `kubectl logs`, `kubectl exec`, obtener mÃ©tricas). Este archivo YAML define un `ClusterRole` (llamado `system:kube-apiserver-to-kubelet`) que otorga los permisos necesarios (acceso a `nodes/proxy`, `nodes/stats`, etc.) y un `ClusterRoleBinding` que asigna ese rol al usuario `kubernetes` (la identidad que usa el `apiserver` cuando se autentica con los `kubelets` usando su certificado cliente).

**CapÃ­tulo 9: Nodos de Trabajo â€“ Donde la AcciÃ³n Sucede (Corresponde al Paso 9 PrÃ¡ctico)**

Los nodos de trabajo son las mÃ¡quinas donde se ejecutan tus aplicaciones.

1.  **Runtime de Contenedores (`containerd` y `runc`):**
    *   **Â¿QuÃ© es un runtime de contenedores?** Es el software responsable de ejecutar y gestionar contenedores en un nodo.
    *   **`containerd`:** Es un runtime de contenedores de alto nivel, un proyecto graduado de la CNCF. Gestiona el ciclo de vida completo del contenedor en su mÃ¡quina host: descarga de imÃ¡genes, gestiÃ³n de almacenamiento y red para contenedores, y supervisiÃ³n de la ejecuciÃ³n. Implementa la **CRI (Container Runtime Interface)**, que es la API que usa el `kubelet` para interactuar con el runtime.
    *   **`runc`:** Es un runtime de contenedores de bajo nivel. `containerd` lo utiliza para realmente crear y ejecutar los contenedores segÃºn la especificaciÃ³n OCI (Open Container Initiative). `runc` se encarga de los detalles de namespaces, cgroups, etc.
    *   **`containerd-config.toml`:** Archivo de configuraciÃ³n para `containerd`.
        *   `plugins."io.containerd.grpc.v1.cri"`: Configura el plugin CRI.
        *   `snapshotter = "overlayfs"`: `overlayfs` es un sistema de archivos de uniÃ³n eficiente para las capas de las imÃ¡genes de contenedor.
        *   `default_runtime_name = "runc"`: Especifica que `runc` es el runtime por defecto.
        *   `SystemdCgroup = true`: Importante para que `containerd` y `kubelet` usen el mismo manejador de `cgroups` (`systemd`), evitando conflictos.
    *   **`crictl`:** Una herramienta de lÃ­nea de comandos para inspeccionar y depurar runtimes compatibles con CRI (como `containerd`).

2.  **Red de Pods (CNI - Container Network Interface):**
    *   **Â¿QuÃ© es CNI?** Es una especificaciÃ³n y un conjunto de librerÃ­as para configurar la red de los contenedores Linux. El `kubelet` invoca plugins CNI para configurar la red de cada Pod.
    *   **`10-bridge.conf` (Plugin `bridge`):**
        *   Crea un puente (bridge) de Linux llamado `cni0`.
        *   Conecta la interfaz de red del Pod (veth pair) a este puente.
        *   Asigna una IP al Pod desde la subred del nodo (`POD_SUBNET`, ej. `10.200.0.0/24`).
        *   `isGateway=true`: Hace que el puente `cni0` actÃºe como la puerta de enlace para los Pods en ese nodo.
        *   `ipMasq=true`: Realiza NAT (Network Address Translation) para el trÃ¡fico que sale de los Pods hacia fuera del nodo, de modo que parezca originarse desde la IP del nodo.
    *   **`99-loopback.conf` (Plugin `loopback`):** Configura la interfaz de red loopback (`lo`) dentro del Pod.
    *   **`modprobe br_netfilter` y `sysctl`:** Estos comandos aseguran que el trÃ¡fico que atraviesa el puente `cni0` sea procesado por `iptables`. Esto es necesario para que funcionen las NetworkPolicies de Kubernetes y para la correcta implementaciÃ³n de los Servicios.
    *   **Desactivar Swap:** Kubernetes espera un entorno de recursos predecible. El swap puede hacer que la contabilidad de memoria sea errÃ¡tica y llevar a un comportamiento inesperado del scheduler y del `kubelet` al aplicar lÃ­mites de memoria.

3.  **`kubelet`:**
    *   **Rol:** Es el agente principal de Kubernetes que se ejecuta en cada nodo de trabajo (y tambiÃ©n podrÃ­a ejecutarse en nodos de control si estos van a correr Pods).
        *   Se registra con el `kube-apiserver`.
        *   Recibe las especificaciones de los Pods (`PodSpecs`) que se le han asignado.
        *   InteractÃºa con el runtime de contenedores (a travÃ©s de CRI) para iniciar, detener y supervisar los contenedores de esos Pods.
        *   Monta los volÃºmenes de los Pods.
        *   Reporta el estado del nodo y de los Pods al `kube-apiserver`.
        *   Realiza health checks de los contenedores.
    *   **`kubelet-config.yaml`:** Su archivo de configuraciÃ³n.
        *   `cgroupDriver: systemd`: Debe coincidir con el `cgroupDriver` de `containerd`.
        *   `containerRuntimeEndpoint: "unix:///var/run/containerd/containerd.sock"`: Le dice al `kubelet` cÃ³mo comunicarse con `containerd`.
        *   `authentication` y `authorization`: Configuran cÃ³mo el `kubelet` se autentica con el `apiserver` (usando su certificado x509) y cÃ³mo se autorizan las solicitudes a la API del `kubelet` (modo `Webhook`, que delega la decisiÃ³n al `apiserver`).
        *   `clientCAFile`: CA para verificar al `apiserver` cuando el `apiserver` se conecta al `kubelet`.
    *   **Certificados y `kubeconfig`:** El `kubelet` usa su propio certificado (ej. `node-0.crt`, `node-0.key`) y `kubeconfig` (ej. `node-0.kubeconfig`) para autenticarse con el `kube-apiserver`. El CN de su certificado (ej. `system:node:node-0`) es usado por el Node Authorizer.

4.  **`kube-proxy`:**
    *   **Rol:** Se ejecuta en cada nodo y es responsable de implementar la abstracciÃ³n de los **Servicios** de Kubernetes.
        *   Observa al `kube-apiserver` para detectar cambios en los objetos `Service` y `Endpoint` (un `Endpoint` es una lista de IPs y puertos de los Pods que respaldan un `Service`).
        *   Mantiene reglas de red en el nodo (en nuestro caso, usando `iptables`) que redirigen el trÃ¡fico destinado a la IP virtual de un `Service` a las IPs reales de los Pods correspondientes.
    *   **`kube-proxy-config.yaml`:**
        *   `kubeconfig`: Para conectarse al `kube-apiserver`.
        *   `mode: "iptables"`: Le dice a `kube-proxy` que use `iptables` para gestionar las reglas de los Servicios. Otras opciones son `ipvs` o `userspace` (obsoleto).
        *   `clusterCIDR`: El rango de IPs general para todos los Pods en el clÃºster. Lo necesita para configurar correctamente `iptables` (ej. para no hacer SNAT al trÃ¡fico entre Pods).

**CapÃ­tulo 10: `kubectl` â€“ Nuestra Interfaz al ClÃºster (Corresponde al Paso 10 PrÃ¡ctico)**

*   **Â¿QuÃ© es `kubectl`?**
    Es la herramienta de lÃ­nea de comandos (CLI) principal para interactuar con un clÃºster de Kubernetes. Permite desplegar aplicaciones, inspeccionar y gestionar recursos del clÃºster, ver logs, etc.
*   **ConfiguraciÃ³n Remota (`~/.kube/config` en el Jumpbox):**
    Al configurar `kubectl` en el `jumpbox` usando el certificado de `admin` y la CA del clÃºster, podemos gestionar el clÃºster remotamente.
    *   `server=https://server.kubernetes.local:6443`: AquÃ­ es crucial que `server.kubernetes.local` sea resoluble desde el `jumpbox` (gracias a la configuraciÃ³n de `/etc/hosts` que hicimos) y que el certificado del `kube-apiserver` sea vÃ¡lido para este nombre (lo es, gracias a los SANs).
*   **Comandos BÃ¡sicos de VerificaciÃ³n:**
    *   `kubectl version`: Muestra la versiÃ³n del cliente (`kubectl`) y del servidor (`kube-apiserver`).
    *   `kubectl get nodes`: Lista los nodos del clÃºster y su estado. Un estado `Ready` indica que el `kubelet` estÃ¡ funcionando correctamente y se ha registrado.

**CapÃ­tulo 11: Red entre Pods â€“ Habilitando la ComunicaciÃ³n (Corresponde al Paso 11 PrÃ¡ctico)**

*   **El Problema de la ComunicaciÃ³n Inter-Nodo:**
    *   Cada nodo tiene su propio rango de IPs para Pods (ej. `node-0` tiene `10.200.0.0/24`, `node-1` tiene `10.200.1.0/24`).
    *   Un Pod en `node-0` (ej. `10.200.0.5`) quiere hablar con un Pod en `node-1` (ej. `10.200.1.7`).
    *   Por defecto, la mÃ¡quina `node-0` no sabe cÃ³mo enrutar trÃ¡fico destinado a la red `10.200.1.0/24`. Su tabla de enrutamiento local no tiene esa informaciÃ³n.
*   **SoluciÃ³n en "The Hard Way" (Rutas EstÃ¡ticas):**
    AÃ±adimos manualmente rutas estÃ¡ticas en cada mÃ¡quina:
    *   En `server`:
        *   Para llegar a `10.200.0.0/24` (Pods en `node-0`), envÃ­a el trÃ¡fico a travÃ©s de la IP de `node-0`.
        *   Para llegar a `10.200.1.0/24` (Pods en `node-1`), envÃ­a el trÃ¡fico a travÃ©s de la IP de `node-1`.
    *   En `node-0`:
        *   Para llegar a `10.200.1.0/24` (Pods en `node-1`), envÃ­a el trÃ¡fico a travÃ©s de la IP de `node-1`.
    *   En `node-1`:
        *   Para llegar a `10.200.0.0/24` (Pods en `node-0`), envÃ­a el trÃ¡fico a travÃ©s de la IP de `node-0`.
*   **Alternativas en ProducciÃ³n (Plugins CNI de Red Overlay/Underlay):**
    Esta configuraciÃ³n manual de rutas no escala y es frÃ¡gil. En producciÃ³n, se utilizan plugins CNI mÃ¡s avanzados que crean una red virtual (overlay network) o se integran con la red fÃ­sica (underlay network) para gestionar este enrutamiento automÃ¡ticamente. Ejemplos: Flannel, Calico, Weave Net, Cilium. Estos plugins se encargan de que cada Pod pueda alcanzar a cualquier otro Pod usando su IP, sin importar en quÃ© nodo se encuentre.

**CapÃ­tulo 12: Smoke Test â€“ Verificando que Todo Funciona (Corresponde al Paso 12 PrÃ¡ctico)**

Este paso es crucial para validar que todos los componentes que hemos configurado interactÃºan correctamente.

1.  **EncriptaciÃ³n de Datos (`Secrets`):**
    *   Al crear un `Secret` y luego inspeccionarlo directamente en `etcd` (usando `etcdctl` y `hexdump`), verificamos que el `kube-apiserver` estÃ¡ usando el `encryption-config.yaml` y que los datos sensibles realmente se almacenan cifrados. El prefijo `k8s:enc:aescbc:v1:key1:` en `etcd` lo confirma.
2.  **Deployments:**
    *   **Â¿QuÃ© es un `Deployment`?** Es un objeto de Kubernetes que proporciona actualizaciones declarativas para Pods y ReplicaSets. Describes el estado deseado en un `Deployment`, y el controlador del `Deployment` cambia el estado actual al estado deseado a una velocidad controlada.
    *   Al crear un `Deployment` de `nginx`, le pedimos a Kubernetes que ejecute una o mÃ¡s instancias (Pods) de la imagen de `nginx`.
3.  **Pods:**
    *   **Â¿QuÃ© es un `Pod`?** Es la unidad de computaciÃ³n mÃ¡s pequeÃ±a y simple que se puede crear y gestionar en Kubernetes. Un Pod representa una instancia de un proceso en ejecuciÃ³n en tu clÃºster. Puede contener uno o mÃ¡s contenedores (como contenedores Docker) que comparten recursos de almacenamiento y red, y una especificaciÃ³n sobre cÃ³mo ejecutar los contenedores.
4.  **Port Forwarding (`kubectl port-forward`):**
    *   Permite acceder a un puerto especÃ­fico de un Pod desde tu mÃ¡quina local (`jumpbox`). `kubectl` crea un tÃºnel de red. Es Ãºtil para depuraciÃ³n y para acceder a aplicaciones que no estÃ¡n expuestas externamente a travÃ©s de un `Service`.
5.  **Logs (`kubectl logs`):**
    *   Recupera los logs (salida estÃ¡ndar y error estÃ¡ndar) de los contenedores dentro de un Pod. Esencial para la depuraciÃ³n.
6.  **Exec (`kubectl exec`):**
    *   Permite ejecutar un comando directamente dentro de un contenedor en ejecuciÃ³n en un Pod. Ãštil para inspeccionar el entorno del contenedor o realizar tareas de diagnÃ³stico.
7.  **Services:**
    *   **Â¿QuÃ© es un `Service`?** Es una abstracciÃ³n que define un conjunto lÃ³gico de Pods y una polÃ­tica para acceder a ellos. Los Servicios permiten un acoplamiento flexible entre los Pods que proporcionan una funcionalidad y los Pods que la consumen. Proporcionan una IP y un puerto estables (y un nombre DNS) para acceder a los Pods, incluso si las IPs de los Pods cambian (porque los Pods son efÃ­meros).
    *   **`kubectl expose deployment nginx --type NodePort`:**
        *   `expose`: Crea un `Service` para un `Deployment` existente.
        *   `--type NodePort`: Este tipo de `Service` expone la aplicaciÃ³n en un puerto estÃ¡tico en la IP de cada nodo del clÃºster. El trÃ¡fico a `NodeIP:NodePort` se redirige al puerto del `Service` y luego a uno de los Pods que respaldan el `Service`.
        *   **LimitaciÃ³n en "The Hard Way":** No tenemos un proveedor de nube integrado, por lo que no podemos usar `type=LoadBalancer` automÃ¡ticamente. `NodePort` es una forma sencilla de obtener acceso externo en este escenario.

**CapÃ­tulo 13: Limpieza â€“ Deshaciendo el Camino (Corresponde al Paso 13 PrÃ¡ctico)**

*   Simplemente eliminar las mÃ¡quinas virtuales es suficiente porque toda la configuraciÃ³n y los datos residen en ellas. No hay estado persistente fuera de las VMs en este tutorial.

---

**Parte IV: Mirando Hacia Adelante â€“ AutomatizaciÃ³n y PrÃ³ximos Pasos**

**CapÃ­tulo 14: AutomatizaciÃ³n â€“ El Camino Inteligente a la ProducciÃ³n**

*   **Â¿Por quÃ© automatizar despuÃ©s de "The Hard Way"?**
    Hacerlo manualmente es educativo, pero para entornos reales, es lento, propenso a errores y difÃ­cil de mantener y replicar.
*   **Herramientas de AutomatizaciÃ³n:**
    *   **Ansible:** Una herramienta de gestiÃ³n de configuraciÃ³n, automatizaciÃ³n de TI y despliegue de aplicaciones. UsarÃ­as playbooks de Ansible para ejecutar los mismos comandos y configuraciones que hicimos manualmente, pero de forma programÃ¡tica y repetible. El proyecto `kubernetes-hard-way-ansible` es un ejemplo directo.
    *   **Terraform:** Una herramienta de Infraestructura como CÃ³digo (IaC). Se usa para provisionar y gestionar la infraestructura subyacente (VMs, redes, balanceadores de carga) en proveedores de nube o locales. Terraform definirÃ­a las 4 mÃ¡quinas, sus redes, etc.
    *   **CombinaciÃ³n:** A menudo se usan juntas. Terraform crea la infraestructura, Ansible la configura.
*   **Otras herramientas de InstalaciÃ³n de Kubernetes:**
    *   **`kubeadm`:** Herramienta oficial de Kubernetes para simplificar la creaciÃ³n de clÃºsteres. Automatiza muchos de los pasos que hicimos manualmente (generaciÃ³n de certificados, configuraciÃ³n de componentes del plano de control, uniÃ³n de nodos).
    *   **k3s, RKE, kops, EKS, GKE, AKS:** Distribuciones ligeras, instaladores o servicios gestionados de Kubernetes que abstraen aÃºn mÃ¡s la complejidad.

**ConclusiÃ³n del Manual TeÃ³rico:**

Al completar "Kubernetes The Hard Way" y comprender la teorÃ­a detrÃ¡s de cada paso, has ganado una visiÃ³n invaluable del funcionamiento interno de Kubernetes. Esta base te permitirÃ¡:

*   **Depurar problemas con mayor eficacia:** Entiendes cÃ³mo interactÃºan los componentes.
*   **Tomar decisiones de diseÃ±o informadas:** Comprendes las implicaciones de diferentes configuraciones.
*   **Apreciar las herramientas de automatizaciÃ³n:** Sabes el trabajo que te estÃ¡n ahorrando.
*   **Continuar aprendiendo con confianza:** Los conceptos mÃ¡s avanzados de Kubernetes se construirÃ¡n sobre esta base sÃ³lida.
